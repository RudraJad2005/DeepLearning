{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac099f0a",
   "metadata": {},
   "source": [
    "# Neural Network Architecture and Hyperparameters\n",
    "\n",
    "## Overview\n",
    "This notebook covers activation functions and network architecture design for different types of machine learning tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Activation Functions\n",
    "\n",
    "### What are Activation Functions?\n",
    "Activation functions introduce **non-linearity** into neural networks, allowing them to learn complex patterns. Without activation functions, neural networks would only be able to learn linear relationships, no matter how many layers you add.\n",
    "\n",
    "### Why are they important?\n",
    "- Enable networks to learn complex, non-linear patterns\n",
    "- Help with gradient flow during backpropagation\n",
    "- Determine the output format (probabilities, classes, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Sigmoid Activation Function\n",
    "\n",
    "### Mathematical Formula:\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "### Properties:\n",
    "- **Output range**: 0 to 1\n",
    "- **Use case**: Binary classification (yes/no, true/false)\n",
    "- **Interpretation**: Output can be interpreted as a probability\n",
    "\n",
    "### When to use Sigmoid:\n",
    "- Binary classification problems (2 classes)\n",
    "- When you need output as probability (0 to 1 range)\n",
    "- Typically used in the **output layer** for binary tasks\n",
    "\n",
    "### Example:\n",
    "```python\n",
    "sigmoid = nn.Sigmoid()\n",
    "probability = sigmoid(input_tensor)  # Output between 0 and 1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Softmax Activation Function\n",
    "\n",
    "### Mathematical Formula:\n",
    "$$\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}$$\n",
    "\n",
    "### Properties:\n",
    "- **Output range**: 0 to 1 for each class\n",
    "- **Sum**: All outputs sum to 1.0\n",
    "- **Use case**: Multi-class classification (choosing one class from many)\n",
    "- **Interpretation**: Probability distribution across classes\n",
    "\n",
    "### When to use Softmax:\n",
    "- Multi-class classification (3+ classes)\n",
    "- When you need probabilities for each class\n",
    "- Typically used in the **output layer** for classification\n",
    "- Use `dim=-1` to apply softmax across the last dimension\n",
    "\n",
    "### Example:\n",
    "```python\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "probabilities = softmax(input_tensor)  # Outputs sum to 1.0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Neural Network Architecture Design\n",
    "\n",
    "### 4.1 Architecture Components:\n",
    "1. **Input Layer**: Size matches number of input features\n",
    "2. **Hidden Layers**: Intermediate processing layers\n",
    "3. **Output Layer**: Size and activation depend on task type\n",
    "\n",
    "### 4.2 Common Architecture Patterns:\n",
    "\n",
    "#### Regression Tasks:\n",
    "- **Output layer size**: 1 (or number of values to predict)\n",
    "- **Output activation**: None (linear output)\n",
    "- **Example**: Predicting house prices, temperature, etc.\n",
    "\n",
    "```python\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(11, 20),  # Input layer\n",
    "    nn.Linear(20, 12),  # Hidden layer\n",
    "    nn.Linear(12, 6),   # Hidden layer\n",
    "    nn.Linear(6, 1),    # Output: 1 value (regression)\n",
    ")\n",
    "```\n",
    "\n",
    "#### Multi-Class Classification:\n",
    "- **Output layer size**: Number of classes\n",
    "- **Output activation**: Softmax\n",
    "- **Example**: Image classification, sentiment analysis\n",
    "\n",
    "```python\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(11, 20),     # Input layer\n",
    "    nn.Linear(20, 12),     # Hidden layer\n",
    "    nn.Linear(12, 6),      # Hidden layer\n",
    "    nn.Linear(6, 4),       # Output: 4 classes\n",
    "    nn.Softmax(dim=-1)     # Convert to probabilities\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Key Design Decisions\n",
    "\n",
    "### Choosing Output Layer Size:\n",
    "- **Binary classification**: 1 neuron + Sigmoid OR 2 neurons + Softmax\n",
    "- **Multi-class classification**: N neurons (N = number of classes) + Softmax\n",
    "- **Regression**: 1 neuron (or number of values to predict) + No activation\n",
    "\n",
    "### Choosing Hidden Layer Sizes:\n",
    "- Start with layers that gradually decrease in size\n",
    "- Common pattern: Input → Larger → Medium → Smaller → Output\n",
    "- No strict rules - requires experimentation\n",
    "\n",
    "### Layer Size Example:\n",
    "```\n",
    "Input (11) → 20 → 12 → 6 → Output (4)\n",
    "```\n",
    "This creates a \"funnel\" pattern, gradually compressing information.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Hyperparameters\n",
    "\n",
    "### What are Hyperparameters?\n",
    "Hyperparameters are settings you choose **before** training:\n",
    "- Number of layers\n",
    "- Number of neurons per layer\n",
    "- Learning rate\n",
    "- Batch size\n",
    "- Number of epochs\n",
    "\n",
    "### Architecture Hyperparameters:\n",
    "1. **Depth**: Number of hidden layers\n",
    "2. **Width**: Number of neurons in each layer\n",
    "3. **Activation functions**: Which functions to use and where\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Quick Reference Table\n",
    "\n",
    "| Task Type | Output Size | Activation | Example |\n",
    "|-----------|-------------|------------|---------|\n",
    "| Binary Classification | 1 | Sigmoid | Spam detection |\n",
    "| Multi-Class Classification | N classes | Softmax | Digit recognition (0-9) |\n",
    "| Regression | 1 or more | None | Price prediction |\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Important Notes\n",
    "\n",
    "⚠️ **Common Mistakes to Avoid:**\n",
    "- Using Softmax for regression (should use no activation)\n",
    "- Wrong output size (must match number of classes)\n",
    "- Forgetting to add activation for classification\n",
    "\n",
    "✅ **Best Practices:**\n",
    "- Match output layer to your task type\n",
    "- Start with simple architectures and add complexity if needed\n",
    "- Use appropriate activation functions for each task\n",
    "- Monitor training to ensure model is learning\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Summary\n",
    "\n",
    "- **Sigmoid**: Binary classification (0 to 1)\n",
    "- **Softmax**: Multi-class classification (probabilities summing to 1)\n",
    "- **Architecture**: Design based on task (regression vs classification)\n",
    "- **Output layer**: Must match task requirements\n",
    "- **Hidden layers**: Experiment with sizes and depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1f3bdcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9168]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "input_tensor = torch.tensor([[2.4]])\n",
    "\n",
    "sigmoid = nn.Sigmoid()\n",
    "probability = sigmoid(input_tensor)\n",
    "print(probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3abf5d4",
   "metadata": {},
   "source": [
    "## Example 1: Sigmoid Activation Function\n",
    "\n",
    "This example demonstrates:\n",
    "- **Input**: A single value (2.4) as a tensor\n",
    "- **Sigmoid function**: Squashes any input to range [0, 1]\n",
    "- **Output**: A probability value between 0 and 1\n",
    "\n",
    "**How it works:**\n",
    "- Input 2.4 is passed through sigmoid: σ(2.4) = 1/(1 + e^(-2.4)) ≈ 0.917\n",
    "- Larger positive values → closer to 1\n",
    "- Larger negative values → closer to 0\n",
    "- Zero → exactly 0.5\n",
    "\n",
    "**Use case**: Binary classification where you need to predict probability of one class (e.g., probability of being spam email)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "680d98f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2828e-01, 1.1698e-04, 5.7492e-01, 3.4961e-02, 1.5669e-01, 1.0503e-01]])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.tensor([[1.0, -6.0, 2.5, -0.3, 1.2, 0.8]])\n",
    "\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "probability = softmax(input_tensor)\n",
    "print(probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e2ea45",
   "metadata": {},
   "source": [
    "## Example 2: Softmax Activation Function\n",
    "\n",
    "This example demonstrates:\n",
    "- **Input**: A tensor with 6 values (representing 6 classes)\n",
    "- **Softmax function**: Converts values to probabilities that sum to 1.0\n",
    "- **dim=-1**: Applies softmax across the last dimension (the 6 values)\n",
    "\n",
    "**How it works:**\n",
    "- Each value is exponentiated: e^x\n",
    "- Then divided by sum of all exponentials\n",
    "- Result: Probability distribution across all classes\n",
    "- The highest input value gets the highest probability\n",
    "\n",
    "**Use case**: Multi-class classification when you have 3+ classes (e.g., classifying images into 6 categories). Each output represents the probability of belonging to that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cc0614e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9888]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# From regression to multi-class classification\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(11, 20),\n",
    "    nn.Linear(20, 12),\n",
    "    nn.Linear(12, 6),\n",
    "    nn.Linear(6, 1),\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66f2edd",
   "metadata": {},
   "source": [
    "## Example 3: Regression Network Architecture\n",
    "\n",
    "This example shows a network designed for **regression tasks**:\n",
    "\n",
    "**Architecture breakdown:**\n",
    "- **Input**: 11 features\n",
    "- **Layer 1**: 11 → 20 neurons (expanding)\n",
    "- **Layer 2**: 20 → 12 neurons (compressing)\n",
    "- **Layer 3**: 12 → 6 neurons (compressing)\n",
    "- **Output**: 6 → 1 neuron (single continuous value)\n",
    "\n",
    "**Key features:**\n",
    "- **No activation function** on output (raw value prediction)\n",
    "- **Funnel architecture**: Gradually reduces dimensions\n",
    "- Output is a continuous value (not bounded)\n",
    "\n",
    "**Use case**: Predicting continuous values like prices, temperatures, distances, or any numerical quantity. The network learns to map 11 input features to 1 output value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a80635f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3251, 0.2706, 0.2406, 0.1637]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# made network below to perform a multi-class classification with four labels.\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(11, 20),\n",
    "  nn.Linear(20, 12),\n",
    "  nn.Linear(12, 6),\n",
    "  nn.Linear(6, 4),\n",
    "  nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d4fe9d",
   "metadata": {},
   "source": [
    "## Example 4: Multi-Class Classification Network\n",
    "\n",
    "This example shows converting the regression network to **multi-class classification**:\n",
    "\n",
    "**Architecture modifications:**\n",
    "- **Input**: Same 11 features\n",
    "- **Hidden layers**: Same structure (11→20→12→6)\n",
    "- **Output layer**: Changed from 1 to **4 neurons** (4 classes)\n",
    "- **Added Softmax**: Converts outputs to probability distribution\n",
    "\n",
    "**Key changes from regression:**\n",
    "1. Output size = number of classes (4)\n",
    "2. Added `nn.Softmax(dim=-1)` to get probabilities\n",
    "3. Each output represents probability of one class\n",
    "4. All outputs sum to 1.0\n",
    "\n",
    "**Output interpretation:**\n",
    "- 4 values, each between 0 and 1\n",
    "- Highest value = most likely class\n",
    "- Example: [0.7, 0.1, 0.15, 0.05] → Class 0 is predicted with 70% confidence\n",
    "\n",
    "**Use case**: Classifying inputs into one of 4 categories (e.g., sentiment analysis with 4 emotions, product categorization with 4 types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "08c0ed7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot Vector using NumPy: [0 1 0]\n",
      "One-hot Vector using PyTorch: tensor([0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "# Creating one-hot encoded labels\n",
    "\n",
    "y = 1\n",
    "num_classes = 3\n",
    "\n",
    "one_hot_numpy = np.array([0, 1, 0])\n",
    "\n",
    "one_hot_pytorch = F.one_hot(torch.tensor(y), num_classes=num_classes)\n",
    "\n",
    "print(\"One-hot Vector using NumPy:\", one_hot_numpy)\n",
    "print(\"One-hot Vector using PyTorch:\", one_hot_pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1e91c45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight of first layer: Parameter containing:\n",
      "tensor([[ 0.1723,  0.2378,  0.1054, -0.0426,  0.0125, -0.2186, -0.2464, -0.0612,\n",
      "          0.2222, -0.1872, -0.1316,  0.1619, -0.1131, -0.0674,  0.0598, -0.0621],\n",
      "        [-0.0280, -0.0523, -0.0172, -0.1929, -0.1212, -0.0987,  0.1360, -0.2201,\n",
      "          0.0828, -0.1478, -0.0396,  0.1866, -0.0482,  0.0894, -0.1385, -0.0157],\n",
      "        [ 0.2057, -0.0845, -0.2106, -0.1561, -0.1500,  0.0995, -0.0650,  0.0982,\n",
      "          0.0675,  0.2314,  0.2010, -0.2123,  0.0730, -0.1607,  0.1091, -0.1293],\n",
      "        [-0.1849,  0.2496,  0.1694, -0.0150,  0.1661,  0.2066,  0.1009,  0.1684,\n",
      "         -0.1981,  0.1089, -0.1963,  0.0232, -0.1143,  0.0834,  0.0576, -0.1283],\n",
      "        [ 0.1495, -0.0761,  0.0874,  0.2147,  0.2308, -0.0480, -0.0643,  0.1971,\n",
      "         -0.1072, -0.1620,  0.0268, -0.1141,  0.0657,  0.0013, -0.0491,  0.0591],\n",
      "        [-0.0696,  0.1303,  0.0989,  0.0194,  0.0238,  0.0427,  0.2092, -0.2401,\n",
      "          0.0374, -0.2434,  0.1693, -0.1067, -0.2210, -0.2145,  0.0742, -0.2379],\n",
      "        [ 0.0020, -0.0182, -0.0771, -0.2391,  0.0463,  0.1353,  0.1924, -0.1670,\n",
      "         -0.0088,  0.0230,  0.0492, -0.2196,  0.0322, -0.0601, -0.0572, -0.2294],\n",
      "        [ 0.1415, -0.2377, -0.0549, -0.2220, -0.1728,  0.1632,  0.2109, -0.0797,\n",
      "         -0.1273,  0.1405,  0.1038,  0.2056,  0.2001, -0.2409,  0.1165,  0.2292]],\n",
      "       requires_grad=True)\n",
      "Bias of second layer: Parameter containing:\n",
      "tensor([ 0.1819, -0.0073], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Accessing the model parameters\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(16, 8),\n",
    "    nn.Linear(8, 2),\n",
    "    nn.Linear(2, 1)\n",
    ")\n",
    "\n",
    "weight0 = model[0].weight\n",
    "print(\"Weight of first layer:\", weight0)\n",
    "\n",
    "bias_1 = model[1].bias\n",
    "print(\"Bias of second layer:\", bias_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ebdb15ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights of first layer: tensor([[ 0.1739,  0.2409,  0.1101, -0.0372,  0.0194, -0.2163, -0.2448, -0.0604,\n",
      "          0.2261, -0.1826, -0.1293,  0.1681, -0.1061, -0.0658,  0.0606, -0.0598],\n",
      "        [-0.0259, -0.0480, -0.0107, -0.1854, -0.1115, -0.0954,  0.1382, -0.2190,\n",
      "          0.0882, -0.1413, -0.0363,  0.1952, -0.0384,  0.0915, -0.1374, -0.0124],\n",
      "        [ 0.2062, -0.0834, -0.2089, -0.1541, -0.1474,  0.1004, -0.0645,  0.0985,\n",
      "          0.0689,  0.2332,  0.2019, -0.2100,  0.0756, -0.1601,  0.1094, -0.1284],\n",
      "        [-0.1923,  0.2348,  0.1472, -0.0409,  0.1328,  0.1955,  0.0935,  0.1647,\n",
      "         -0.2166,  0.0868, -0.2074, -0.0064, -0.1476,  0.0761,  0.0539, -0.1394],\n",
      "        [ 0.1440, -0.0872,  0.0708,  0.1953,  0.2059, -0.0563, -0.0699,  0.1944,\n",
      "         -0.1210, -0.1786,  0.0185, -0.1362,  0.0408, -0.0043, -0.0518,  0.0508],\n",
      "        [-0.0702,  0.1291,  0.0972,  0.0174,  0.0212,  0.0419,  0.2086, -0.2404,\n",
      "          0.0360, -0.2451,  0.1685, -0.1090, -0.2235, -0.2150,  0.0739, -0.2388],\n",
      "        [-0.0025, -0.0272, -0.0906, -0.2548,  0.0260,  0.1285,  0.1879, -0.1692,\n",
      "         -0.0201,  0.0095,  0.0424, -0.2375,  0.0120, -0.0645, -0.0594, -0.2362],\n",
      "        [ 0.1418, -0.2372, -0.0541, -0.2210, -0.1716,  0.1636,  0.2112, -0.0796,\n",
      "         -0.1266,  0.1412,  0.1042,  0.2067,  0.2013, -0.2406,  0.1166,  0.2296]],\n",
      "       grad_fn=<SubBackward0>)\n",
      "Loss: 1.538509488105774\n"
     ]
    }
   ],
   "source": [
    "# First, perform a forward pass and backward pass to compute gradients\n",
    "input_tensor = torch.Tensor([[2, 4, 6, 7, 9, 3, 2, 1, 5, 6, 3, 8, 9, 2, 1, 3]])\n",
    "target = torch.Tensor([[1.0]])\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_tensor)\n",
    "\n",
    "# Compute loss (mean squared error)\n",
    "loss = ((output - target) ** 2).mean()\n",
    "\n",
    "# Backward pass to compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Now access the gradients\n",
    "weight0 = model[0].weight\n",
    "weight1 = model[1].weight\n",
    "weight2 = model[2].weight\n",
    "\n",
    "grads0 = weight0.grad\n",
    "grads1 = weight1.grad\n",
    "grads2 = weight2.grad\n",
    "\n",
    "# Update the weights using the learning rate and the gradients\n",
    "learning_rate = 0.01\n",
    "\n",
    "weight0 = weight0 - learning_rate * grads0\n",
    "weight1 = weight1 - learning_rate * grads1\n",
    "weight2 = weight2 - learning_rate * grads2\n",
    "\n",
    "print(\"Updated weights of first layer:\", weight0)\n",
    "\n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "205e5a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the PyTorch optimizer\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
