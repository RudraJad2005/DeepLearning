{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "954d74af",
   "metadata": {},
   "source": [
    "# Training Robust Neural Networks — Notes\n",
    "\n",
    "## Notebook Overview\n",
    "- Goal: Binary classification of water potability with a small feedforward neural network in PyTorch, trained and evaluated with accuracy.\n",
    "- Data: Custom `WaterDataset` reads CSVs, features are last column excluded, label is last column.\n",
    "\n",
    "## Cell-by-Cell Notes\n",
    "- **Cell 1 — Imports:** Loads `pandas`, `torch`, `Dataset`, `DataLoader`, `nn`, `optim`, and `torchmetrics.Accuracy` for data, modeling, optimization, and evaluation.\n",
    "- **Cell 2 — `WaterDataset`:** Reads CSV to NumPy; returns `torch.Tensor` features/labels (`float32`). Label reshaped later with `view(-1, 1)` to match model output.\n",
    "- **Cell 3 — Train DataLoader + Preview:** Wraps the dataset (`batch_size=2`, `shuffle=True`); prints a sample batch to verify shapes/dtypes.\n",
    "- **Cell 4 — `Net` model:** `fc1(9→16)` → ReLU → `fc2(16→8)` → ReLU → `fc(8→1)` → Sigmoid to output a probability for binary classification.\n",
    "- **Cell 5 — Training:** `train_model(...)` runs a standard loop with `BCELoss`. Casts tensors to float, reshapes labels to `(-1, 1)`, and uses `SGD(lr=0.001)`. Prints epoch progress.\n",
    "- **Cell 6 — Evaluation:** Builds test `DataLoader`, sets `net.eval()`, uses `torch.no_grad()`, thresholds predictions at 0.5, computes `Accuracy(task=\"binary\")`, prints final accuracy.\n",
    "\n",
    "## Key Flow\n",
    "- CSV → `WaterDataset` → `DataLoader` (train/test) → model → train with BCE → evaluate with accuracy.\n",
    "\n",
    "## Pitfalls Avoided\n",
    "- Dtype/shape mismatches: Explicit `.float()` casts and `labels.view(-1, 1)` ensure `BCELoss` matches model output.\n",
    "- Correct modes: `net.eval()` and `torch.no_grad()` for evaluation to disable gradients.\n",
    "\n",
    "## Recommended Improvements\n",
    "- Numerical stability: Prefer `BCEWithLogitsLoss` and remove model `sigmoid`; threshold logits with `torch.sigmoid` in eval.\n",
    "- Feature scaling: Normalize/standardize inputs to improve convergence.\n",
    "- Optimizer/params: Try `Adam(lr=1e-3)`, tune `batch_size` (e.g., 32) and `num_epochs`.\n",
    "- Metrics: Add precision/recall/F1 if class imbalance exists (`torchmetrics`).\n",
    "- Reproducibility: Set seeds for `torch` and `numpy`; control `DataLoader` randomness.\n",
    "\n",
    "## Run Order\n",
    "1. Imports\n",
    "2. Dataset\n",
    "3. Train DataLoader (preview)\n",
    "4. Model\n",
    "5. Training\n",
    "6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07061eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchmetrics import Accuracy\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed271147",
   "metadata": {},
   "source": [
    "### Notes — Imports\n",
    "- pandas: CSV loading and data manipulation.\n",
    "- torch: Core tensor and autograd library.\n",
    "- torch.utils.data `Dataset`, `DataLoader`: Custom dataset wrapper and batch iteration.\n",
    "- torch.nn, torch.nn.functional: Neural network layers and functional activations (ReLU, Sigmoid).\n",
    "- torch.optim: Optimizers (SGD, Adam) for parameter updates.\n",
    "- torchmetrics `Accuracy(task=\"binary\")`: Computes binary classification accuracy from predictions and targets.\n",
    "\n",
    "Run this cell first to make all symbols available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8541a184",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaterDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file):\n",
    "        super().__init__()\n",
    "\n",
    "        df = pd.read_csv(csv_file)\n",
    "        self.data = df.to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features = self.data[idx, :-1].astype('float32')\n",
    "        labels = self.data[idx, -1].astype('float32')\n",
    "        return torch.from_numpy(features), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158f6e4b",
   "metadata": {},
   "source": [
    "### Notes — WaterDataset\n",
    "- Reads a CSV into a NumPy array and stores it in `self.data`.\n",
    "- `__len__`: returns number of rows.\n",
    "- `__getitem__`: splits features (all columns except last) and label (last column).\n",
    "- Casts to `float32` and returns PyTorch tensors for seamless training.\n",
    "- Label is a scalar; later reshaped to `(-1, 1)` to match model output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dff8032d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2634, 0.2375, 0.3439, 0.5421, 0.5010, 0.4620, 0.5912, 0.6320, 0.5630],\n",
      "        [0.7376, 0.5225, 0.3144, 0.3796, 0.5965, 0.2441, 0.4886, 0.4642, 0.6881]]) tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the WaterDataset\n",
    "\n",
    "dataset_train = WaterDataset(csv_file='./water_potability/water_train.csv')\n",
    "\n",
    "data_loader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size = 2,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "features, labels = next(iter(data_loader_train))\n",
    "print(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1d2796",
   "metadata": {},
   "source": [
    "### Notes — Train DataLoader & Preview\n",
    "- Wraps the training dataset in a `DataLoader` with `batch_size=2` and `shuffle=True` for stochastic minibatches.\n",
    "- The preview (`next(iter(...))`) quickly checks shapes/dtypes of a sample batch.\n",
    "- Ensure features are tensors of shape `[batch, 9]` and labels are `[batch]` (reshaped later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "323023f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(9, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc = nn.Linear(8, 1)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.sigmoid(self.fc(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4ae2ef",
   "metadata": {},
   "source": [
    "### Notes — Model (`Net`)\n",
    "- Architecture: `fc1(9→16)` → ReLU → `fc2(16→8)` → ReLU → `fc(8→1)` → Sigmoid.\n",
    "- Final Sigmoid outputs a probability for binary classification.\n",
    "- Pairing with `BCELoss` is consistent; alternatively remove Sigmoid and use `BCEWithLogitsLoss` for better numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db72c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 completed\n",
      "Epoch 2/10 completed\n",
      "Epoch 3/10 completed\n",
      "Epoch 4/10 completed\n",
      "Epoch 5/10 completed\n",
      "Epoch 6/10 completed\n",
      "Epoch 7/10 completed\n",
      "Epoch 8/10 completed\n",
      "Epoch 9/10 completed\n",
      "Epoch 10/10 completed\n"
     ]
    }
   ],
   "source": [
    "# Training Loop.\n",
    "\n",
    "# for epoch in range(1000):\n",
    "#     for features, labels in data_loader_train:\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = criterion(net(features), labels.view(-1, 1)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "def train_model(optimizer, net, num_epochs, criterion=None, data_loader=None):\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    data_loader = data_loader_train\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for features, labels in data_loader:\n",
    "            features = features.float()\n",
    "            labels = labels.float()\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(net(features), labels.view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} completed\")\n",
    "        \n",
    "###### Optimizers ######\n",
    "net = Net()\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "train_model(\n",
    "    optimizer=optimizer,\n",
    "    net=net,\n",
    "    num_epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab18e79b",
   "metadata": {},
   "source": [
    "### Notes — Training Loop\n",
    "- `train_model(...)`: Standard epoch/batch loop.\n",
    "- Loss: `BCELoss` expects probabilities; model applies Sigmoid.\n",
    "- Casting: `features.float()` and `labels.float()` avoid dtype issues.\n",
    "- Shape: `labels.view(-1, 1)` matches model output shape `[batch, 1]`.\n",
    "- Optimizer: SGD with learning rate `0.001` updates parameters each step.\n",
    "- Prints epoch completion for progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7def37b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5904572606086731\n"
     ]
    }
   ],
   "source": [
    "dataset_test = WaterDataset(csv_file='./water_potability/water_test.csv')\n",
    "\n",
    "data_loader_test = DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size = 2,\n",
    "    shuffle=True\n",
    ")\n",
    "# Ignore above code (its just for context)\n",
    "########### Model evaluation ###########\n",
    "acc = Accuracy(task=\"binary\")\n",
    "\n",
    "net.eval()  \n",
    "with torch.no_grad():\n",
    "    for features, labels in data_loader_test:\n",
    "\n",
    "        output = net(features)\n",
    "        preds = (output > 0.5).float()\n",
    "        acc(preds, labels.view(-1, 1))\n",
    "\n",
    "test_accuracy = acc.compute()\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90839054",
   "metadata": {},
   "source": [
    "### Notes — Evaluation\n",
    "- Test `DataLoader` mirrors training setup.\n",
    "- `net.eval()` + `torch.no_grad()` disable dropout (if any) and gradient tracking.\n",
    "- Forward pass: `output` is probability; threshold at `0.5` to get binary predictions.\n",
    "- Metric: `Accuracy(task=\"binary\")` computes final accuracy; `acc.compute()` returns a scalar.\n",
    "- Prints test accuracy for quick performance assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079cecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch normalization and He initialization\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(9, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        \n",
    "\n",
    "        init.kaiming_uniform_(self.fc1.weight)\n",
    "        init.kaiming_uniform_(self.fc2.weight)\n",
    "        init.kaiming_uniform_(self.fc3.weight, nonlinearity=\"sigmoid\")\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = nn.functional.elu(self.fc1(x))\n",
    "        x = nn.functional.elu(self.fc2(x))\n",
    "        x = nn.functional.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81851564",
   "metadata": {},
   "source": [
    "### Notes — ELU Model with He Initialization\n",
    "- Defines `Net` with three linear layers: `fc1(9→16)`, `fc2(16→8)`, `fc3(8→1)`.\n",
    "- Activations: ELU on hidden layers (`fc1`, `fc2`), Sigmoid on output (`fc3`) to produce a probability for binary classification.\n",
    "- Initialization: He/Kaiming uniform on `fc1` and `fc2` suits ELU/ReLU. Output layer uses Kaiming with `nonlinearity=\"sigmoid\"`; for Sigmoid outputs, Xavier often works better (`init.xavier_uniform_(fc3.weight)`).\n",
    "- Shapes & loss: Output is `[batch, 1]`; pair with `nn.BCELoss`. If you remove Sigmoid, switch to `nn.BCEWithLogitsLoss` and apply `torch.sigmoid` for thresholding at evaluation.\n",
    "- Usage: Create `net = Net()` after this cell, retrain with your training loop, then evaluate with `net.eval()` and `torch.no_grad()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f5d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Updated Model with Batch Normalization and He Initialization ############### \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(9, 16)\n",
    "        self.bn1 = nn.BatchNorm1d(16) # Batch Normalization layer after first FC layer\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.bn2 = nn.BatchNorm1d(8)  # Batch Normalization layer after second FC layer\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        \n",
    "        init.kaiming_uniform_(self.fc1.weight)\n",
    "        init.kaiming_uniform_(self.fc2.weight)\n",
    "        init.kaiming_uniform_(self.fc3.weight, nonlinearity=\"sigmoid\") # He initialization\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)  # First set of layer\n",
    "        x = self.bn1(x)\n",
    "        x = nn.functional.elu(x)\n",
    "\n",
    "\n",
    "        x = self.fc2(x) # Second set of layer\n",
    "        x = self.bn2(x)\n",
    "        x = nn.functional.elu(x)\n",
    "\n",
    "        x = nn.functional.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5071e6",
   "metadata": {},
   "source": [
    "### Notes — BatchNorm + He Initialization Model\n",
    "- Active architecture: `fc1(9→16)` → `BatchNorm1d(16)` → ELU → `fc2(16→8)` → `BatchNorm1d(8)` → ELU → `fc3(8→1)` → Sigmoid.\n",
    "- BatchNorm: stabilizes training and speeds convergence; in `net.eval()` it uses running statistics. Ensure batch size > 1 (your `batch_size=2` is fine).\n",
    "- Initialization: He/Kaiming on hidden layers fits ELU/ReLU. For the sigmoid output, consider Xavier (`init.xavier_uniform_(fc3.weight)`) if training is unstable.\n",
    "- Loss pairing: With final Sigmoid, use `nn.BCELoss`. Without Sigmoid, use `nn.BCEWithLogitsLoss` and threshold logits via `torch.sigmoid` during evaluation.\n",
    "- Shapes: Inputs `[batch, 9]` → outputs `[batch, 1]`; reshape labels in training/eval with `labels.view(-1, 1)`.\n",
    "- Tips: With very small batches, BatchNorm stats can be noisy; alternatives include `LayerNorm` or `GroupNorm`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
