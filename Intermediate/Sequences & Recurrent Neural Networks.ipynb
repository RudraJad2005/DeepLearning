{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b6b8c48",
   "metadata": {},
   "source": [
    "# To Generate sequential data\n",
    "\n",
    "\n",
    "To be able to train neural networks on sequential data, we need to pre-process it first. we'll chunk the data into inputs-target pairs, where the inputs are some number of consecutive data points and the target is the next data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fce89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_sequences(df, seq_length):\n",
    "    data = df['consumption'].values\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Example usage:\n",
    "# X, y = create_sequences(df, seq_length=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4762dd81",
   "metadata": {},
   "source": [
    "### Sequential Dataset\n",
    "\n",
    "Let's create a training Dataset and DataLoader.To build a sequential Dataset, you will call `create_sequences()` to get the NumPy arrays with inputs and targets, and inspect their shape. Next, you will pass them to a `TensorDataset` to create a proper torch Dataset, and inspect its length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abf04cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (105119, 96)\n",
      "y_train shape: (105119,)\n",
      "Number of samples in dataset_train: 105119\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv('./electricity_consump/electricity_train.csv')\n",
    "\n",
    "# Check if 'consumption' column exists\n",
    "assert 'consumption' in train_data.columns, \"Column 'consumption' not found in DataFrame\"\n",
    "\n",
    "# Create sequences\n",
    "seq_length = 24 * 4  # 4 days of hourly data, adjust as needed\n",
    "X_train, y_train = create_sequences(train_data, seq_length)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "# Ensure shapes are compatible for TensorDataset\n",
    "assert len(X_train) == len(y_train), \"Input and target lengths do not match\"\n",
    "\n",
    "dataset_train = TensorDataset(\n",
    "    torch.tensor(X_train).float(),\n",
    "    torch.tensor(y_train).float()\n",
    ")\n",
    "print(\"Number of samples in dataset_train:\", len(dataset_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bfb218",
   "metadata": {},
   "source": [
    "### Building a forecasting RNN\n",
    "\n",
    "It's time to build our first recurrent network! It will be a sequence-to-vector model consisting of an RNN layer with two layers and a `hidden_size` of 32. After the RNN layer, a simple linear layer will map the outputs to a single value to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64b10573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size = 1,\n",
    "            hidden_size = 32,\n",
    "            num_layers = 2,\n",
    "            batch_first = True,\n",
    "        )\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(2, x.size(0), 32)\n",
    "\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e2481b",
   "metadata": {},
   "source": [
    "#### LSTM network\n",
    "\n",
    "As we already know, plain RNN cells are not used that much in practice. A more frequently used alternative that ensures a much better handling of long sequences are Long Short-Term Memory cells, or LSTMs.\n",
    "\n",
    "The most important implementation difference from the RNN network we have built previously comes from the fact that LSTMs have two rather than one hidden states. This means we will need to initialize this additional hidden state and pass it to the LSTM cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5030ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1,\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        h0 = torch.zeros(2, x.size(0), 32)\n",
    "        c0 = torch.zeros(2, x.size(0), 32)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fc9633",
   "metadata": {},
   "source": [
    "#### GRU network\n",
    "\n",
    "Next to LSTMs, another popular recurrent neural network variant is the Gated Recurrent Unit, or GRU. It's appeal is in its simplicity: GRU cells require less computation than LSTM cells while often matching them in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1549bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=1,\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(2, x.size(0), 32)\n",
    "        out, _ = self.gru(x, h0)  \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
