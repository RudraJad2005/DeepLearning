{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c81c3911",
   "metadata": {},
   "source": [
    "# Evaluating and Improving Models\n",
    "\n",
    "## Overview\n",
    "This notebook covers techniques for evaluating neural network performance and improving model training through various methods including freezing layers, transfer learning, and optimization strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e74855dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=8, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=8, out_features=5, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=5, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "Model parameters:\n",
      "0.weight: shape torch.Size([8, 10]), requires_grad=True\n",
      "0.bias: shape torch.Size([8]), requires_grad=True\n",
      "2.weight: shape torch.Size([5, 8]), requires_grad=True\n",
      "2.bias: shape torch.Size([5]), requires_grad=True\n",
      "4.weight: shape torch.Size([2, 5]), requires_grad=True\n",
      "4.bias: shape torch.Size([2]), requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import torchmetrics\n",
    "\n",
    "# Create a Sequential model with multiple layers\n",
    "# This allows us to access layers by index (0, 1, 2, etc.)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 8),   # Layer 0: Input layer\n",
    "    nn.ReLU(),          # Layer 1: Activation\n",
    "    nn.Linear(8, 5),    # Layer 2: Hidden layer\n",
    "    nn.ReLU(),          # Layer 3: Activation\n",
    "    nn.Linear(5, 2)     # Layer 4: Output layer\n",
    ")\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "print(model)\n",
    "print(\"\\nModel parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: shape {param.shape}, requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fcdc93",
   "metadata": {},
   "source": [
    "### Understanding Layer Freezing\n",
    "\n",
    "**How it works:**\n",
    "- `param.requires_grad = True`: Parameter will be updated during training\n",
    "- `param.requires_grad = False`: Parameter is frozen (no updates)\n",
    "\n",
    "**Layer naming in Sequential models:**\n",
    "- Layers are indexed: `0`, `1`, `2`, etc.\n",
    "- Each layer has parameters: `weight` and `bias`\n",
    "- Full name format: `{layer_index}.{parameter_name}`\n",
    "- Example: `'0.weight'` = weight of first layer, `'2.bias'` = bias of third layer\n",
    "\n",
    "**Common freezing patterns:**\n",
    "```python\n",
    "# Freeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Freeze only first N layers\n",
    "for i, (name, param) in enumerate(model.named_parameters()):\n",
    "    if i < N * 2:  # *2 because each layer has weight and bias\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Freeze by name pattern\n",
    "for name, param in model.named_parameters():\n",
    "    if 'conv' in name:  # Freeze all convolutional layers\n",
    "        param.requires_grad = False\n",
    "```\n",
    "\n",
    "**Use case:** In transfer learning, you freeze early layers (that detect basic features) and only train the later layers (that learn task-specific patterns)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02090cde",
   "metadata": {},
   "source": [
    "## Example 1: Freezing Model Layers\n",
    "\n",
    "**What is Layer Freezing?**\n",
    "Freezing layers means preventing certain layers from being updated during training by setting `requires_grad=False` on their parameters.\n",
    "\n",
    "**Why Freeze Layers?**\n",
    "- **Transfer Learning**: Keep pre-trained weights from earlier layers unchanged\n",
    "- **Faster Training**: Only update weights in unfrozen layers\n",
    "- **Prevent Overfitting**: Preserve learned features from pre-trained models\n",
    "- **Fine-tuning**: Adapt pre-trained models to new tasks with limited data\n",
    "\n",
    "**When to Freeze Layers:**\n",
    "- Using a pre-trained model on a similar task\n",
    "- Training on a small dataset\n",
    "- Early layers have learned useful general features\n",
    "- Fine-tuning only the last few layers for a new task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6af3bf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen: 0.weight\n",
      "Frozen: 2.weight\n",
      "\n",
      "After freezing:\n",
      "0.weight: requires_grad=False\n",
      "0.bias: requires_grad=True\n",
      "2.weight: requires_grad=False\n",
      "2.bias: requires_grad=True\n",
      "4.weight: requires_grad=True\n",
      "4.bias: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "# Freeze layers by checking parameter names\n",
    "for name, param in model.named_parameters():\n",
    "    # Check for first layer's weight (layer 0)\n",
    "    if name == '0.weight':\n",
    "        # Freeze this weight\n",
    "        param.requires_grad = False\n",
    "        print(f\"Frozen: {name}\")\n",
    "    \n",
    "    # Check for second layer's weight (layer 2)\n",
    "    if name == '2.weight':\n",
    "        # Freeze this weight\n",
    "        param.requires_grad = False\n",
    "        print(f\"Frozen: {name}\")\n",
    "\n",
    "print(\"\\nAfter freezing:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6775ee",
   "metadata": {},
   "source": [
    "## Example 2: Weight Initialization\n",
    "\n",
    "This example demonstrates:\n",
    "- **Creating layers** with `nn.Linear`\n",
    "- **Initializing weights** using `nn.init.uniform_()`\n",
    "- **Building a model** with initialized layers\n",
    "\n",
    "**What is Weight Initialization?**\n",
    "Weight initialization is the process of setting initial values for the weights in a neural network before training begins. Proper initialization is crucial for:\n",
    "- Faster convergence during training\n",
    "- Avoiding vanishing/exploding gradients\n",
    "- Breaking symmetry (ensuring neurons learn different features)\n",
    "\n",
    "**Uniform Initialization:**\n",
    "$$\\text{Uniform}(a, b): \\text{weights} \\sim U(a, b)$$\n",
    "\n",
    "The `nn.init.uniform_()` function:\n",
    "- Initializes weights from a uniform distribution\n",
    "- Default range: `[-1/√n, 1/√n]` where n = number of input features\n",
    "- The `_` suffix means it modifies weights **in-place**\n",
    "\n",
    "**Code breakdown:**\n",
    "```python\n",
    "layer0 = nn.Linear(16, 32)              # Create layer\n",
    "nn.init.uniform_(layer0.weight)          # Initialize its weights\n",
    "```\n",
    "\n",
    "**Common Initialization Methods:**\n",
    "\n",
    "1. **Uniform Distribution**: `nn.init.uniform_(tensor, a, b)`\n",
    "   - Random values uniformly distributed between a and b\n",
    "   - Use: General purpose, simple initialization\n",
    "\n",
    "2. **Normal Distribution**: `nn.init.normal_(tensor, mean, std)`\n",
    "   - Random values from normal distribution\n",
    "   - Use: When you want values clustered around mean\n",
    "\n",
    "3. **Xavier/Glorot**: `nn.init.xavier_uniform_(tensor)`\n",
    "   - Maintains variance of activations across layers\n",
    "   - Use: With sigmoid or tanh activations\n",
    "\n",
    "4. **He/Kaiming**: `nn.init.kaiming_uniform_(tensor)`\n",
    "   - Designed for ReLU activations\n",
    "   - Use: With ReLU or LeakyReLU (most common in modern networks)\n",
    "\n",
    "5. **Constant**: `nn.init.constant_(tensor, value)`\n",
    "   - Sets all weights to same value\n",
    "   - Use: Usually for biases (often initialized to 0)\n",
    "\n",
    "**Why Initialize Weights?**\n",
    "- **Prevent symmetry**: If all weights start the same, all neurons learn the same features\n",
    "- **Scale properly**: Too large → exploding gradients, too small → vanishing gradients\n",
    "- **Faster training**: Good initialization helps the model learn faster\n",
    "\n",
    "**Best Practices:**\n",
    "- Use **He initialization** for ReLU networks (most common)\n",
    "- Use **Xavier initialization** for sigmoid/tanh networks\n",
    "- Initialize **biases to zero** or small constants\n",
    "- Don't use all zeros or all same values\n",
    "\n",
    "**Note:** PyTorch automatically initializes weights when you create a layer, but you can override this with custom initialization for better performance on specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "241e5999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "  (1): Linear(in_features=32, out_features=64, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Using uniform initialization for layer0 and layer1 weights\n",
    "\n",
    "layer0 = nn.Linear(16, 32)\n",
    "layer1 = nn.Linear(32, 64)\n",
    "\n",
    "nn.init.uniform_(layer0.weight)\n",
    "nn.init.uniform_(layer1.weight)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    layer0,\n",
    "    layer1\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1a834c",
   "metadata": {},
   "source": [
    "## Example 3: Model Validation\n",
    "\n",
    "This example demonstrates:\n",
    "- **Creating a validation dataset** from a pandas DataFrame\n",
    "- **Model evaluation mode** using `model.eval()`\n",
    "- **Validation loop** with `torch.no_grad()`\n",
    "- **Computing validation loss** without updating weights\n",
    "\n",
    "**What is Model Validation?**\n",
    "Validation is the process of evaluating a trained model on unseen data to assess its performance and generalization ability.\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "1. **`model.eval()`**: \n",
    "   - Puts model in evaluation mode\n",
    "   - Disables dropout and batch normalization layers (if present)\n",
    "   - Important for consistent predictions\n",
    "\n",
    "2. **`torch.no_grad()`**:\n",
    "   - Disables gradient computation\n",
    "   - Reduces memory usage\n",
    "   - Speeds up computation (no need to track gradients)\n",
    "   - Essential during validation/inference\n",
    "\n",
    "3. **Validation Loss Calculation**:\n",
    "   - Forward pass through model\n",
    "   - Compute loss using criterion\n",
    "   - Accumulate loss over all batches\n",
    "   - **No backward pass or optimizer step**\n",
    "\n",
    "**Validation vs Training:**\n",
    "- **Training**: `model.train()` + gradients + optimizer updates\n",
    "- **Validation**: `model.eval()` + no gradients + no updates\n",
    "\n",
    "**The Dataset:**\n",
    "- 4 animals with features: Height and Weight\n",
    "- Target: Age prediction\n",
    "- Features (X): Height, Weight (2 features)\n",
    "- Labels (y): Age (1 output)\n",
    "\n",
    "**Why Validation is Important:**\n",
    "- Check for overfitting (model memorizing training data)\n",
    "- Compare different model architectures\n",
    "- Tune hyperparameters (learning rate, batch size, etc.)\n",
    "- Decide when to stop training (early stopping)\n",
    "\n",
    "**Common Practice:**\n",
    "Split your data into:\n",
    "- **Training set** (70-80%): Used to train the model\n",
    "- **Validation set** (10-15%): Used to tune and evaluate during training\n",
    "- **Test set** (10-15%): Final evaluation on completely unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b895eb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total validation loss: 0.8905611485242844\n"
     ]
    }
   ],
   "source": [
    "animals = pd.DataFrame({\n",
    "\t'Name': ['Dog', 'Cat', 'Bird', 'Fish'],\n",
    "\t'Height': [60, 25, 15, 5],\n",
    "\t'Weight': [30, 5, 0.5, 0.1],\n",
    "\t'Age': [5, 3, 2, 1]\n",
    "})\n",
    "\n",
    "\n",
    "X = animals.iloc[:, 1:-1].to_numpy().astype(float)\n",
    "y = animals.iloc[:, -1].to_numpy().astype(float)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 8),  \n",
    "    nn.Linear(8, 4),\n",
    "    nn.Linear(4, 1)\n",
    ")\n",
    "dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float())\n",
    "validationloader = DataLoader(dataset, batch_size=3, shuffle=False)  # shuffle=False for consistent results\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for data in validationloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        features, targets = data\n",
    "        features = features.float()\n",
    "        targets = targets.float()\n",
    "        prediction = model(features)\n",
    "        loss = criterion(prediction.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "############# ########### IGNORE ABOVE CODE FOR NOW########################\n",
    "\n",
    "model.eval()\n",
    "\n",
    "validation_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for features, labels in validationloader:\n",
    "\n",
    "        output = model(features).squeeze(-1)  # Only squeeze last dimension\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        validation_loss += loss.item()\n",
    "\n",
    "validation_loss_epoch = validation_loss / len(validationloader)\n",
    "print(validation_loss_epoch)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3f182b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2500\n"
     ]
    }
   ],
   "source": [
    "# Classification dataset (Height, Weight) -> Class (0=Small, 1=Medium, 2=Large)\n",
    "X_class = torch.tensor([[60, 30], [25, 5], [15, 0.5], [5, 0.1]]).float()\n",
    "y_class = torch.tensor([2, 1, 0, 0])  # Class labels: 0, 1, 2\n",
    "\n",
    "# Classification model (3 outputs for 3 classes)\n",
    "torch.manual_seed(42)\n",
    "class_model = nn.Sequential(\n",
    "    nn.Linear(2, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 3)\n",
    ")\n",
    "\n",
    "class_dataset = TensorDataset(X_class, y_class)\n",
    "class_loader = DataLoader(class_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# Accuracy metric\n",
    "class_model.eval()\n",
    "\n",
    "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3)\n",
    "with torch.no_grad():\n",
    "    for features, labels in class_loader:\n",
    "        output = class_model(features)\n",
    "        metric.update(output, labels)\n",
    "\n",
    "accuracy = metric.compute()\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "metric.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
