{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72f03fa4",
   "metadata": {},
   "source": [
    "# Training a Neural Network\n",
    "\n",
    "## Overview\n",
    "This notebook covers the essential components of training a neural network in PyTorch, including data loading, loss functions, and the complete training loop.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Data Preparation\n",
    "\n",
    "### Why Data Preparation Matters\n",
    "Before training a neural network, you need to organize your data into a format that PyTorch can efficiently process. This involves:\n",
    "- Converting data to tensors\n",
    "- Creating datasets\n",
    "- Batching data for efficient training\n",
    "- Shuffling data to prevent learning order-dependent patterns\n",
    "\n",
    "### Key Components:\n",
    "- **TensorDataset**: Wraps tensors to create a dataset\n",
    "- **DataLoader**: Handles batching, shuffling, and iteration over data\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Loss Functions\n",
    "\n",
    "### What is a Loss Function?\n",
    "A loss function (also called cost function or objective function) measures how far the model's predictions are from the actual target values. The goal of training is to **minimize this loss**.\n",
    "\n",
    "### Common Loss Functions:\n",
    "- **Mean Squared Error (MSE)**: Used for regression tasks\n",
    "  - Formula: $\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$\n",
    "  - Measures average squared difference between predictions and targets\n",
    "  \n",
    "- **Cross Entropy Loss**: Used for classification tasks\n",
    "  - Measures difference between predicted and true probability distributions\n",
    "\n",
    "### Why MSE for Regression?\n",
    "- Penalizes larger errors more heavily (due to squaring)\n",
    "- Smooth and differentiable (good for gradient descent)\n",
    "- Easy to interpret (same units as the target squared)\n",
    "\n",
    "---\n",
    "\n",
    "## 3. The Training Loop\n",
    "\n",
    "### Core Training Steps:\n",
    "1. **Zero Gradients**: Clear previous gradients with `optimizer.zero_grad()`\n",
    "2. **Forward Pass**: Pass input through model to get predictions\n",
    "3. **Compute Loss**: Calculate how wrong the predictions are\n",
    "4. **Backward Pass**: Compute gradients with `loss.backward()`\n",
    "5. **Update Weights**: Apply gradients to update parameters with `optimizer.step()`\n",
    "\n",
    "### Why Zero Gradients?\n",
    "PyTorch **accumulates** gradients by default. If you don't zero them, gradients from previous iterations will be added to current ones, causing incorrect updates.\n",
    "\n",
    "### Training Over Epochs:\n",
    "- **Epoch**: One complete pass through the entire training dataset\n",
    "- **Batch**: A subset of the training data processed together\n",
    "- Multiple epochs allow the model to see the data multiple times and learn better\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Optimizers\n",
    "\n",
    "### What is an Optimizer?\n",
    "An optimizer implements the algorithm for updating model parameters based on computed gradients.\n",
    "\n",
    "### Common Optimizers:\n",
    "- **SGD (Stochastic Gradient Descent)**: Basic optimizer\n",
    "- **Adam**: Adaptive learning rate, works well in most cases\n",
    "- **RMSprop**: Good for recurrent neural networks\n",
    "\n",
    "### Optimizer Workflow:\n",
    "```python\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "# In training loop:\n",
    "optimizer.zero_grad()  # Clear gradients\n",
    "loss.backward()        # Compute gradients\n",
    "optimizer.step()       # Update parameters\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Training Workflow Summary\n",
    "\n",
    "```\n",
    "For each epoch:\n",
    "    For each batch in dataloader:\n",
    "        1. Zero gradients\n",
    "        2. Forward pass (get predictions)\n",
    "        3. Calculate loss\n",
    "        4. Backward pass (compute gradients)\n",
    "        5. Update parameters\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Key PyTorch Classes & Methods\n",
    "\n",
    "### Data Handling:\n",
    "- `TensorDataset(X, y)`: Create dataset from tensors\n",
    "- `DataLoader(dataset, batch_size, shuffle)`: Create data loader\n",
    "\n",
    "### Training:\n",
    "- `criterion = nn.MSELoss()`: Create loss function\n",
    "- `optimizer = optim.SGD(model.parameters(), lr)`: Create optimizer\n",
    "- `optimizer.zero_grad()`: Clear gradients\n",
    "- `loss.backward()`: Compute gradients\n",
    "- `optimizer.step()`: Update parameters\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Important Concepts\n",
    "\n",
    "### Batch Size:\n",
    "- **Small batches** (e.g., 32): \n",
    "  - More frequent updates\n",
    "  - More noise in gradient estimates\n",
    "  - Better generalization\n",
    "  \n",
    "- **Large batches** (e.g., 256):\n",
    "  - Fewer updates per epoch\n",
    "  - More stable gradients\n",
    "  - Faster computation on GPU\n",
    "\n",
    "### Shuffling:\n",
    "- Randomizes order of samples each epoch\n",
    "- Prevents model from learning order of data\n",
    "- Improves generalization\n",
    "\n",
    "### Learning Rate:\n",
    "- Controls how much to adjust weights based on gradients\n",
    "- Too high: Model may not converge (overshoots minimum)\n",
    "- Too low: Training is very slow\n",
    "- Typical values: 0.001 to 0.1\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Quick Reference\n",
    "\n",
    "| Component | Purpose | Example |\n",
    "|-----------|---------|---------|\n",
    "| TensorDataset | Package data and labels | `TensorDataset(X_tensor, y_tensor)` |\n",
    "| DataLoader | Batch and iterate data | `DataLoader(dataset, batch_size=32)` |\n",
    "| Loss Function | Measure prediction error | `nn.MSELoss()` |\n",
    "| Optimizer | Update parameters | `optim.SGD(model.parameters(), lr=0.01)` |\n",
    "| Training Loop | Learn from data | See examples below |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841e983b",
   "metadata": {},
   "source": [
    "## Example 1: Creating a Dataset with TensorDataset\n",
    "\n",
    "This example demonstrates:\n",
    "- **Extracting features and labels** from a pandas DataFrame (`animals`)\n",
    "- **Converting to tensors** using `torch.tensor()`\n",
    "- **Creating a TensorDataset** to package inputs and labels together\n",
    "- **Accessing samples** from the dataset using indexing\n",
    "\n",
    "**Key components:**\n",
    "- `X`: Feature matrix (all columns except first and last)\n",
    "- `y`: Target labels (last column)\n",
    "- `TensorDataset`: Combines X and y into a single dataset object\n",
    "- Indexing with `dataset[0]` retrieves the first sample and its label\n",
    "\n",
    "**Why use TensorDataset?**\n",
    "- Automatically pairs inputs with their corresponding labels\n",
    "- Makes it easy to iterate through data\n",
    "- Works seamlessly with DataLoader for batching\n",
    "\n",
    "**Note**: This assumes you have a DataFrame called `animals` loaded in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893411c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import \n",
    "\n",
    "# Create a sample animals DataFrame\n",
    "animals = pd.DataFrame({\n",
    "\t'Name': ['Dog', 'Cat', 'Bird', 'Fish'],\n",
    "\t'Height': [60, 25, 15, 5],\n",
    "\t'Weight': [30, 5, 0.5, 0.1],\n",
    "\t'Age': [5, 3, 2, 1]\n",
    "})\n",
    "\n",
    "X = animals.iloc[:, 1:-1].to_numpy()  \n",
    "y = animals.iloc[:, -1].to_numpy()\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(torch.tensor(X), torch.tensor(y))\n",
    "\n",
    "# Print the first sample\n",
    "input_sample, label_sample = dataset[0]\n",
    "print('Input sample:', input_sample)\n",
    "print('Label sample:', label_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8c95f4",
   "metadata": {},
   "source": [
    "## Example 2: Creating a DataLoader for Batch Processing\n",
    "\n",
    "This example demonstrates:\n",
    "- **Creating a DataLoader** from the dataset\n",
    "- **Setting batch size**: `batch_size=2` means 2 samples per batch\n",
    "- **Shuffling**: `shuffle=True` randomizes sample order each iteration\n",
    "- **Iterating through batches**: The loop automatically provides batched data\n",
    "\n",
    "**How DataLoader works:**\n",
    "- Splits the dataset into batches of specified size\n",
    "- Each iteration returns a batch of inputs and a batch of labels\n",
    "- Automatically handles the last batch (which may be smaller)\n",
    "\n",
    "**Parameters explained:**\n",
    "- `batch_size=2`: Process 2 samples at a time\n",
    "- `shuffle=True`: Randomize order to prevent learning patterns from data order\n",
    "\n",
    "**Benefits of batching:**\n",
    "- More efficient computation (especially on GPU)\n",
    "- Provides regularization effect (prevents overfitting)\n",
    "- Allows training on datasets larger than memory (by loading one batch at a time)\n",
    "\n",
    "**Output:** Each iteration prints one batch of inputs and their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf13aab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size = 2, shuffle=True)\n",
    "\n",
    "# Iterate over the dataloader\n",
    "for batch_inputs, batch_labels in dataloader:\n",
    "    print('batch_inputs:', batch_inputs)\n",
    "    print('batch_labels:', batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1aa465",
   "metadata": {},
   "source": [
    "## Example 3: Computing Mean Squared Error (MSE) Loss\n",
    "\n",
    "This example demonstrates:\n",
    "- **Calculating MSE manually** using NumPy\n",
    "- **Calculating MSE using PyTorch** with `nn.MSELoss()`\n",
    "- **Comparing both methods** to verify they produce the same result\n",
    "\n",
    "**Mean Squared Error Formula:**\n",
    "$$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_{\\text{pred}} - y_{\\text{true}})^2$$\n",
    "\n",
    "**How it works:**\n",
    "1. Compute the difference between predictions and true values\n",
    "2. Square each difference (to make all values positive and penalize large errors)\n",
    "3. Take the mean of all squared differences\n",
    "\n",
    "**Why use MSE?**\n",
    "- Standard loss function for regression problems\n",
    "- Differentiable (needed for backpropagation)\n",
    "- Penalizes larger errors more heavily due to squaring\n",
    "- Easy to interpret (lower is better)\n",
    "\n",
    "**PyTorch vs NumPy:**\n",
    "- NumPy version: Manual calculation using `np.mean((y_pred - y)**2)`\n",
    "- PyTorch version: Built-in `nn.MSELoss()` function\n",
    "- Both produce identical results\n",
    "\n",
    "**Use case:** This loss function is used during training to measure how well the model's predictions match the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fe2cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([3, 5.0, 2.5, 7.0])  \n",
    "y = np.array([3.0, 4.5, 2.0, 8.0])     \n",
    "\n",
    "# Calculate MSE using NumPy\n",
    "mse_numpy = np.mean((y_pred - y)**2)\n",
    "\n",
    "# Create the MSELoss function in PyTorch\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Calculate MSE using PyTorch\n",
    "mse_pytorch = criterion(torch.tensor(y_pred), torch.tensor(y))\n",
    "\n",
    "print(\"MSE (NumPy):\", mse_numpy)\n",
    "print(\"MSE (PyTorch):\", mse_pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0bd42f",
   "metadata": {},
   "source": [
    "## Example 4: Complete Training Loop\n",
    "\n",
    "This example demonstrates the **full training workflow** for a neural network:\n",
    "\n",
    "**Training Loop Structure:**\n",
    "```\n",
    "For each epoch (complete pass through dataset):\n",
    "    For each batch in dataloader:\n",
    "        1. optimizer.zero_grad()    ← Clear previous gradients\n",
    "        2. prediction = model(feature)  ← Forward pass\n",
    "        3. loss = criterion(prediction, target)  ← Compute loss\n",
    "        4. loss.backward()         ← Compute gradients (backpropagation)\n",
    "        5. optimizer.step()        ← Update weights\n",
    "```\n",
    "\n",
    "**Step-by-step breakdown:**\n",
    "\n",
    "1. **Outer loop (`range(num_epochs)`)**: \n",
    "   - Iterates through multiple complete passes of the training data\n",
    "   - More epochs = more learning opportunities\n",
    "\n",
    "2. **Inner loop (`for data in dataloader`)**: \n",
    "   - Processes one batch at a time\n",
    "   - `feature, target = data` unpacks batch inputs and labels\n",
    "\n",
    "3. **`optimizer.zero_grad()`**: \n",
    "   - **CRITICAL**: Clears old gradients\n",
    "   - Without this, gradients accumulate and cause wrong updates\n",
    "\n",
    "4. **Forward pass (`prediction = model(feature)`)**: \n",
    "   - Passes input through the network\n",
    "   - Gets predicted output\n",
    "\n",
    "5. **Loss computation (`loss = criterion(prediction, target)`)**: \n",
    "   - Measures how wrong predictions are\n",
    "   - Uses MSE or other loss function defined as `criterion`\n",
    "\n",
    "6. **Backward pass (`loss.backward()`)**: \n",
    "   - Computes gradients for all parameters\n",
    "   - Uses automatic differentiation (autograd)\n",
    "   - Gradients stored in `.grad` attribute of each parameter\n",
    "\n",
    "7. **Parameter update (`optimizer.step()`)**: \n",
    "   - Updates all model parameters using computed gradients\n",
    "   - Applies learning rule (SGD, Adam, etc.)\n",
    "   - Formula: `weight = weight - learning_rate * gradient`\n",
    "\n",
    "**After training:**\n",
    "- `show_results(model, dataloader)` displays the results\n",
    "- Model has learned from the data and can make predictions\n",
    "\n",
    "**Prerequisites for this code:**\n",
    "- `model`: A defined neural network\n",
    "- `criterion`: Loss function (e.g., `nn.MSELoss()`)\n",
    "- `optimizer`: Optimization algorithm (e.g., `torch.optim.SGD()`)\n",
    "- `num_epochs`: Number of times to iterate through dataset\n",
    "- `dataloader`: DataLoader with training data\n",
    "\n",
    "**Key concept:** This loop is the heart of deep learning - it's how neural networks learn from data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18c7362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(16, 8),\n",
    "    nn.Linear(8, 4),\n",
    "    nn.Linear(4, 1)\n",
    ")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Set number of epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Loop over the number of epochs and the dataloader\n",
    "for i in range(num_epochs):\n",
    "  for data in dataloader:\n",
    "    # Set the gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "    # Run a forward pass\n",
    "    feature, target = data\n",
    "    prediction = model(feature)    \n",
    "    # Compute the loss\n",
    "    loss = criterion(prediction, target)    \n",
    "    # Compute the gradients\n",
    "    loss.backward()\n",
    "    # Update the model's parameters\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"Training complete! Final loss: {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
